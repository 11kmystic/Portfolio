---
title: "Heart Disease Classification ML"
author: "KW"
date: '2022-12-01'
output:
  rmdformats::readthedown:
  self_contained: true
  thumbnails: true
  lightbox: true
  gallery: false
  highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(corrplot)
library(dplyr)
library(e1071)
library(kernlab)
library(ggplot2)
library(pROC)
library(randomForest)
library(tidyverse)
set.seed(169)
```

Let's begin by loading in our data. [Found here](https://www.kaggle.com/datasets/ketangangal/heart-disease-dataset-uci)

```{r}
df <- read.csv("HeartDiseaseTrain-Test.csv")
summary(df)
```
## Data Exploration

Extract all unique values of all character columns to see what cleaning could be done and begin to think about how to
interpret our data

```{r}
sexes <- unique(df$sex)
blood_sugar <- unique(df$fasting_blood_sugar)
ang <- unique(df$exercise_induced_angina)
slope <-unique(df$slope)
vessel <-unique(df$vessels_colored_by_flourosopy)
thal <-unique(df$thalassemia)
sexes
blood_sugar
ang
slope
vessel
thal
```
This looks pretty good. The most important changge we'll make is turning the 0s in target to Negative and the 1s to Positive. 
I'll also change vessel values from characters to integers, and some of the labels to be stand alone


```{r}
#Allows for interactions
ves <- as.list(df$vessels_colored_by_flourosopy)

#New vector to replace old values
new_ves <- c()
for (i in ves) {
  if (i == "Two"){
    new_ves <- c(new_ves, 2)
  }
  else if (i == "Zero") {
    new_ves <- c(new_ves, 0)
  }
  else if (i == "One") {
    new_ves <- c(new_ves, 1)
  }
  else if (i == "Three") {
    new_ves <- c(new_ves, 3)
  }
  else {
    new_ves <- c(new_ves, 4) #assuming "Four" is the last unique value
  }
}



df$vessels_colored_by_flourosopy <- new_ves

# Changes 0 to Negative, 1 to Positive, and factors characters for easier use
data <- df %>% mutate(target = if_else( target ==1, "Positive", "Negative")) %>%
  mutate_if(is.character, as.factor)
  
```



## Data Visualization

```{r}
ggplot(data, aes(x=target, fill=target))+
   geom_bar()+
   xlab("Heart Disease")+
   ylab("count")+
   ggtitle("Presence & Absence of Heart Disease")+
  theme(plot.title = element_text(hjust = 0.5)) +
   scale_fill_discrete(name= 'Heart Disease', labels =c("Absence", "Presence"))
```

Here we see that there is a near 50-50 split between diseased and healthy samples, making this a fine data set to use, let's continue to investigate some of the key variables to look for trends

## Variable Investigation

Let's compare various features by target results

```{r}
data %>%
  group_by( age) %>%
  count() %>%
  ggplot()+
  geom_col(aes(x = age, y = n), fill = 'orange')+
  ggtitle("Age Analysis")+
  xlab("Age")+
  ylab("Count")+
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
sd(df$age)
mean(df$age)
```


```{r}
data %>%
  ggplot(aes(x=sex, y= resting_blood_pressure))+
  geom_boxplot(fill ='purple')+
  xlab('sex')+
  ylab('Resting Blood Presure')+
  ggtitle("Angina Instances vs Resting Blood Pressure") +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_grid(~chest_pain_type)
```

Here we see that angina instances are more typical in those with lower resting blood pressure, matching a stidu which can be found [here](https://pubmed.ncbi.nlm.nih.gov/30190000/)


```{r}
data %>%
  ggplot(aes(x=sex, y= cholestoral))+
  geom_boxplot(fill ='green')+
  xlab('sex')+
  ylab('Cholesterol')+
  ggtitle("Cholesterol vs Chest Pain") +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_grid(~chest_pain_type)
```

While cholesterol levels varied, with a couple of outliers present in this dataset

We can also look at possible correlations between numerical variables, which we know the names od from our intial investigation

```{r}
num_variables <- c("age","resting_blood_pressure","cholestoral","Max_heart_rate","oldpeak")

corr <- cor(data[, num_variables])

corrplot(corr, method = 'square', type = 'upper')
```

Other than a weak positive correlation between age and max heart rate, things don't look super related

Now since that's all done, let's begin to build an ML model for our data. We will compare 3 different models

## Train and Test Data

```{r}
partition<- createDataPartition(data$target, p =0.7, list = FALSE)
train_data <- data[partition,]
test_data <- data[-partition,]
```

## Logistic Regression

```{r}
AUC <- list()
accuracy <- list()

log_reg_model <- train (target ~ ., data=train_data, method = 'glm', # For distrubution other than normal
                        family = 'binomial')
log_reg_prediction <- predict(log_reg_model, test_data)
log_reg_prediction_prob <- predict(log_reg_model, test_data, type='prob')[2]
log_reg_confusion <- confusionMatrix(log_reg_prediction, test_data[,"target"])

#ROC Curve
AUC$logReg <- roc(as.numeric(test_data$target),as.numeric(as.matrix((log_reg_prediction_prob))))$auc
accuracy$logReg <- as.numeric(log_reg_confusion$overall['Accuracy'])  #found names with str
```

## Support Vector Machine

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)

svm <- train(target ~ ., data = train_data,
             method = "svmRadial",
             trControl = fitControl,  
             preProcess = c("center", "scale"), #correspond to chosen method
             tuneLength = 8,
             metric = "ROC")

svmPrediction <- predict(svm, test_data)
svmPredictionprob <- predict(svm, test_data, type='prob')[2]
svmConfMat <- confusionMatrix(svmPrediction, test_data[,"target"])
#ROC Curve
AUC$svm <- roc(as.numeric(test_data$target),as.numeric(as.matrix((svmPredictionprob))))$auc
accuracy$svm <- as.numeric(svmConfMat$overall['Accuracy'])
```

## Random Forest

```{r}
RFModel <- randomForest(target ~ .,
                    data=train_data, 
                    importance=TRUE, 
                    ntree=200)
#varImpPlot(RFModel)
RFPrediction <- predict(RFModel, test_data)
RFPredictionprob = predict(RFModel,test_data,type="prob")[, 2]

RFConfMat <- confusionMatrix(RFPrediction, test_data[,"target"])

AUC$RF <- roc(as.numeric(test_data$target),as.numeric(as.matrix((RFPredictionprob))))$auc
accuracy$RF <- as.numeric(RFConfMat$overall['Accuracy'])  
```

## Final Comparison

```{r}
row.names <- names(accuracy)
col.names <- c("AUC", "Accuracy")
results_table <- cbind(as.data.frame(matrix(c(AUC,accuracy),nrow = 3, ncol = 2,
                           dimnames = list(row.names, col.names))))
results_table
```

Our results indicated Random Forest to have gotten the best results. Let's analyze further

```{r}
log_reg_confusion
svmConfMat
RFConfMat
```

Here we see that the Random Forest method did give the best accuracy with 0.9804 while logistic regression gave relatively lower accuracy with 0.8529